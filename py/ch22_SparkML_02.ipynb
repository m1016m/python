{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ch22_SparkML_02.ipynb\n",
    "#Spark ML Pipeline 回歸分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'local[*]'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1.準備資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step1.1 \n",
    "import pandas as pd\n",
    "pandas_df=pd.read_csv(\"hour.csv\")             #以pandas.read_csv() 將\"hour.csv\"讀入成 pandas Dataframe 'pandas_df'\n",
    "hour_df=sqlContext.createDataFrame(pandas_df)    #以sqlContext.createDataframe() 將pandas_df 轉成spark Dataframe hour_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17379"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hour_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['instant', 'dteday', 'season', 'yr', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'casual', 'registered', 'cnt']\n"
     ]
    }
   ],
   "source": [
    "print(hour_df.columns) #查看欄位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step1.1a , 或者這樣作:\n",
    "hour_df= spark.read.format('csv').option(\"header\", 'true').load(\"hour.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step1.2 以Dataframe.drop() 捨棄不需要的欄位, instant,dateday,yr,casual,registered\n",
    "hour_df=hour_df.drop(\"instant\").drop(\"dteday\") \\\n",
    "                            .drop('yr').drop(\"casual\").drop(\"registered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: string (nullable = true)\n",
      " |-- mnth: string (nullable = true)\n",
      " |-- hr: string (nullable = true)\n",
      " |-- holiday: string (nullable = true)\n",
      " |-- weekday: string (nullable = true)\n",
      " |-- workingday: string (nullable = true)\n",
      " |-- weathersit: string (nullable = true)\n",
      " |-- temp: string (nullable = true)\n",
      " |-- atemp: string (nullable = true)\n",
      " |-- hum: string (nullable = true)\n",
      " |-- windspeed: string (nullable = true)\n",
      " |-- cnt: string (nullable = true)\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#step1.3 檢視schema, 所有欄位datatype都是string, 我們稍候要將datatype 轉為double\n",
    "print hour_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step1.4 匯入 col module, 以col模組讀取欄位資料\n",
    "from pyspark.sql.functions import col  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step1.5 將datatype 轉為double\n",
    "hour_df= hour_df.select([col(column).cast(\"double\").alias(column)\n",
    "                        for column in hour_df.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- season: double (nullable = true)\n",
      " |-- mnth: double (nullable = true)\n",
      " |-- hr: double (nullable = true)\n",
      " |-- holiday: double (nullable = true)\n",
      " |-- weekday: double (nullable = true)\n",
      " |-- workingday: double (nullable = true)\n",
      " |-- weathersit: double (nullable = true)\n",
      " |-- temp: double (nullable = true)\n",
      " |-- atemp: double (nullable = true)\n",
      " |-- hum: double (nullable = true)\n",
      " |-- windspeed: double (nullable = true)\n",
      " |-- cnt: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hour_df.printSchema() #欄位datatype 已轉為 double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+\n",
      "|season|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed| cnt|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+\n",
      "|   1.0| 1.0|0.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.81|      0.0|16.0|\n",
      "|   1.0| 1.0|1.0|    0.0|    6.0|       0.0|       1.0|0.22|0.2727| 0.8|      0.0|40.0|\n",
      "|   1.0| 1.0|2.0|    0.0|    6.0|       0.0|       1.0|0.22|0.2727| 0.8|      0.0|32.0|\n",
      "|   1.0| 1.0|3.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.75|      0.0|13.0|\n",
      "|   1.0| 1.0|4.0|    0.0|    6.0|       0.0|       1.0|0.24|0.2879|0.75|      0.0| 1.0|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hour_df.show(5) #查看結果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[season: double, mnth: double, hr: double, holiday: double, weekday: double, workingday: double, weathersit: double, temp: double, atemp: double, hum: double, windspeed: double, cnt: double]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step1.6  將資料依7:3分成訓練資料集(train_df)及測試資料集(test_df)\n",
    "#              並以 DataFrame.cache() 將資料暫存在 MM\n",
    "train_df, test_df = hour_df.randomSplit([0.7, 0.3])\n",
    "train_df.cache()\n",
    "test_df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#2. 建立Spark Machine Learning pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step2.1 匯入所需模組,Pipeline,StringIndexer,VectorIndexer,VectorAssembler, DesisionTreeRegressor\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import  StringIndexer,  VectorIndexer,VectorAssembler\n",
    "from pyspark.ml.regression import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed']\n"
     ]
    }
   ],
   "source": [
    "#step2.2 建立feature欄位 list, hour_df 只有最後欄位'cnt'是label(count per hour)\n",
    "#              所以我們的featuer欄位從前面到倒數第二個 [:-1]\n",
    "featuresCols = hour_df.columns[:-1]\n",
    "print(featuresCols) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step2.3 建立 pipeline\n",
    "#建立 VectorAssembler 物件 'vectorAssembler'\n",
    "vectorAssembler = VectorAssembler(inputCols=featuresCols, outputCol=\"aFeatures\")\n",
    "#建立 VectorIndexer 物件 'vectorIndexer', mnth,hr,weekday轉成分類feature欄位\n",
    "vectorIndexer = VectorIndexer(inputCol=\"aFeatures\", outputCol=\"features\", maxCategories=24) \n",
    "#建立 DecisionTreeRegressor 物件 'dt'\n",
    "dt = DecisionTreeRegressor(labelCol=\"cnt\",featuresCol= 'features')\n",
    "#建立 Pipeline 物件 dt_pipeline, stages=vectorAssembler,vectorIndexer ,dt\n",
    "dt_pipeline = Pipeline(stages=[vectorAssembler,vectorIndexer ,dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[VectorAssembler_4169864eb47385bca5bf,\n",
       " VectorIndexer_45d4aef2fc51134af20c,\n",
       " DecisionTreeRegressor_40adba5c5a1ddeb66291]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step2.4 以getStages() 檢視 Pipeline stages\n",
    "dt_pipeline .getStages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3. 使用 Pipeline 進行資料處理與訓練"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step3.1 以 Pipeline.fit(), 參數為 DataFrame train_df 進行訓練\n",
    "#             訓練結果為 pyspark.ml.pipeline.PipelineModel  物件 'df_pipelineModel'\n",
    "dt_pipelineModel = dt_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_40adba5c5a1ddeb66291) of depth 5 with 63 nodes"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step3.2 檢視訓練後DecisionTree Model, 位於pipeline 最後一個 stage\n",
    "dt_pipelineModel.stages[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_40adba5c5a1ddeb66291) of depth 5 with 63 nodes\n",
      "  If (feature 2 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,22.0,23.0})\n",
      "   If (feature 2 in {0.0,1.0,2.0,3.0,4.0,5.0})\n",
      "    If (feature 2 in {2.0,3.0,4.0,5.0})\n",
      "     If (feature 2 in {3.0,4.0})\n",
      "      If (feature 4 in {1.0,2.0,3.0,4.0,5.0})\n",
      "       Predict: 5.345375722543353\n",
      "      Else (feature 4 not in {1.0,2.0,3.0,4.0,5.0})\n",
      "       Predict: 17.24671052631579\n",
      "     Else (feature 2 not in {3.0,4.0})\n",
      "      If (fea\n"
     ]
    }
   ],
   "source": [
    "#step3.3 檢視練後DecisionTree Model規則 (只顯示前500筆) .toDebugString 屬性\n",
    "print dt_pipelineModel.stages[-1].toDebugString[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#4. 使用pipelineModel 進行預測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step4.1 以PipelineModel.transform(), 參數為 DataFrame test_df 進行預測\n",
    "#              預測結果 為 DataFrame 'predicted_df'\n",
    "predicted_df=dt_pipelineModel.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', 'weathersit', 'temp', 'atemp', 'hum', 'windspeed', 'cnt', 'aFeatures', 'features', 'prediction']\n"
     ]
    }
   ],
   "source": [
    "#step4.2 檢視預測結果 'preficted_df' 的欄位, 注意:新增 'prediction' 欄位\n",
    "print predicted_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "|season|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed| cnt|        prediction|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.16|0.1818| 0.8|   0.1045|33.0|  59.6554054054054|\n",
      "|   1.0| 1.0|0.0|    0.0|    1.0|       1.0|       1.0|0.06|0.0606|0.41|    0.194| 7.0|36.475935828877006|\n",
      "|   1.0| 1.0|0.0|    0.0|    1.0|       1.0|       1.0|0.22| 0.197|0.44|   0.3582| 5.0|36.475935828877006|\n",
      "|   1.0| 1.0|0.0|    0.0|    1.0|       1.0|       1.0|0.24|0.2273| 0.6|   0.2239|15.0|36.475935828877006|\n",
      "|   1.0| 1.0|0.0|    0.0|    1.0|       1.0|       2.0|0.32|0.2879|0.26|   0.4179|10.0|36.475935828877006|\n",
      "|   1.0| 1.0|0.0|    0.0|    2.0|       1.0|       2.0|0.22|0.2424|0.87|   0.1045|14.0|36.475935828877006|\n",
      "|   1.0| 1.0|0.0|    0.0|    3.0|       1.0|       1.0|0.08|0.0606|0.42|   0.3284| 9.0|36.475935828877006|\n",
      "|   1.0| 1.0|0.0|    0.0|    3.0|       1.0|       1.0|0.26| 0.303|0.93|      0.0|31.0|36.475935828877006|\n",
      "|   1.0| 1.0|0.0|    0.0|    3.0|       1.0|       2.0|0.22|0.2273|0.69|   0.1343|17.0|36.475935828877006|\n",
      "|   1.0| 1.0|0.0|    0.0|    4.0|       1.0|       2.0|0.28|0.2879|0.75|   0.1045|25.0|36.475935828877006|\n",
      "|   1.0| 1.0|0.0|    0.0|    4.0|       1.0|       3.0|0.34|0.3333|0.93|    0.194| 3.0|36.475935828877006|\n",
      "|   1.0| 1.0|0.0|    0.0|    5.0|       1.0|       1.0|0.12|0.1364| 0.5|    0.194|14.0|36.475935828877006|\n",
      "|   1.0| 1.0|0.0|    0.0|    5.0|       1.0|       2.0|0.26|0.2273|0.48|   0.2985|27.0|36.475935828877006|\n",
      "|   1.0| 1.0|0.0|    0.0|    5.0|       1.0|       2.0| 0.4|0.4091|0.71|   0.2836|42.0|36.475935828877006|\n",
      "|   1.0| 1.0|0.0|    0.0|    6.0|       0.0|       1.0|0.04|0.0303|0.45|   0.2537|13.0|  59.6554054054054|\n",
      "|   1.0| 1.0|0.0|    0.0|    6.0|       0.0|       3.0|0.16|0.1515|0.64|   0.2239|24.0|  59.6554054054054|\n",
      "|   1.0| 1.0|0.0|    1.0|    1.0|       0.0|       1.0|0.14|0.1515|0.46|   0.1343|25.0|36.475935828877006|\n",
      "|   1.0| 1.0|1.0|    0.0|    0.0|       0.0|       1.0|0.26|0.2727|0.56|   0.1343|23.0|  59.6554054054054|\n",
      "|   1.0| 1.0|1.0|    0.0|    0.0|       0.0|       2.0|0.16|0.1364|0.47|   0.2985|42.0|  59.6554054054054|\n",
      "|   1.0| 1.0|1.0|    0.0|    0.0|       0.0|       2.0|0.44|0.4394|0.94|   0.2537|17.0|  59.6554054054054|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#step4.3 檢視預測結果, not good!\n",
    "predicted_df.select('season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', \\\n",
    "                     'weathersit', 'temp', 'atemp', 'hum', 'windspeed','cnt','prediction').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#5. 評估模型的準確率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step5.1 匯入模組 RegressionEvaluator\n",
    "from pyspark.ml.evaluation import RegressionEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step5.2 建立RegressionEvaluator 物件 evaluator, 以RMSE 評估\n",
    "evaluator = RegressionEvaluator(labelCol='cnt',\n",
    "                                                        predictionCol='prediction',\n",
    "                                                        metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92.51461487831746"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step5.3 計算 RMSE\n",
    "predicted_df=dt_pipelineModel.transform(test_df)\n",
    "rmse = evaluator.evaluate(predicted_df)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#6. 使用TrainValidation進行訓練評估找出最佳模型\n",
    "#    ML Tuning: model selection and hyperparameter tuning\n",
    "#    Train-Validation Split\n",
    "#    https://spark.apache.org/docs/2.1.0/ml-tuning.html#train-validation-split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step6.1 匯入模組 ParamGridBuilder,\n",
    "from pyspark.ml.tuning import ParamGridBuilder,TrainValidationSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step6.2 設定訓練驗證(Train Validation) 的參數\n",
    "#使用 ParamGridBuilder() \n",
    "#設定 maxDepth 4種\n",
    "#設定 maxBins 4種\n",
    "#所以執行訓練驗證時會執行 4*4=16次\n",
    "paramGrid = ParamGridBuilder()\\\n",
    "  .addGrid(dt.maxDepth, [ 5,10,15,25])\\\n",
    "  .addGrid(dt.maxBins, [25,35,45,50])\\\n",
    "  .build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step6.3 建立TrainValidationSplit 物件 tvs\n",
    "tvs = TrainValidationSplit(estimator=dt,evaluator=evaluator,    #以之前建立的 DesisionTreeRegressor 'dt' 作為 estimator\n",
    "                  estimatorParamMaps=paramGrid,trainRatio=0.8)    #訓練:8 與測試:2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step6.4 建立 Pipeline 'tvs_pipeline'\n",
    "tvs_pipeline = Pipeline(stages=[vectorAssembler,vectorIndexer ,tvs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step6.5 以Pipeline 'tvs_pipeline 對DataFrame 'train_df'進行驗證訓練,產生 Model 'tvs_pipelineModel'\n",
    "tvs_pipelineModel =tvs_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_40adba5c5a1ddeb66291) of depth 10 with 1753 nodes\n",
      "  If (feature 2 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,22.0,23.0})\n",
      "   If (feature 2 in {0.0,1.0,2.0,3.0,4.0,5.0})\n",
      "    If (feature 2 in {2.0,3.0,4.0,5.0})\n",
      "     If (feature 2 in {3.0,4.0})\n",
      "      If (feature 4 in {1.0,2.0,3.0,4.0,5.0})\n",
      "       If (feature 1 in {0.0,1.0,2.0,3.0,11.0})\n",
      "        If (feature 7 <= 0.4)\n",
      "         If (feature 0 in {0.0,1.0})\n",
      "          If (feature 7 <= 0.2)\n",
      "           If (feature\n"
     ]
    }
   ],
   "source": [
    "#step6.6 檢視訓練後最佳模型(bestModel) \n",
    "bestModel=tvs_pipelineModel.stages[2].bestModel\n",
    "print bestModel.toDebugString[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+\n",
      "| cnt|        prediction|\n",
      "+----+------------------+\n",
      "|33.0| 44.42857142857143|\n",
      "| 7.0|14.411764705882353|\n",
      "| 5.0|14.411764705882353|\n",
      "|15.0|              26.5|\n",
      "|10.0|              26.5|\n",
      "|14.0| 8.545454545454545|\n",
      "| 9.0| 8.545454545454545|\n",
      "|31.0|              17.5|\n",
      "|17.0| 8.545454545454545|\n",
      "|25.0|              17.5|\n",
      "| 3.0|               3.0|\n",
      "|14.0|14.411764705882353|\n",
      "|27.0|              26.5|\n",
      "|42.0|49.029411764705884|\n",
      "|13.0|22.666666666666668|\n",
      "|24.0|              51.0|\n",
      "|25.0|14.411764705882353|\n",
      "|23.0| 51.44444444444444|\n",
      "|42.0|              51.0|\n",
      "|17.0|              17.0|\n",
      "+----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#step6.7 以tvs_pipelineModel.transform() 對測試資料 'test_df' 作預測, 並檢視部份預測結果\n",
    "predictions=tvs_pipelineModel.transform(test_df)\n",
    "predictions.select('cnt','prediction').show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.88845514741912"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step6.8 評估最佳模型 RMSE, 有好一點\n",
    "rmse= evaluator.evaluate(predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#7. 使用crossValidation進行交叉訓練找出最佳模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step7.1 匯入模組 CrossValidator\n",
    "from pyspark.ml.tuning import CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step7.2 建立 CrossValidator 物件 'cv', k=3; 'cv2', k=10\n",
    "#              k=3時會執行 4*4*3=48次, k=10時會執行4*4*10=160次\n",
    "cv = CrossValidator(estimator=dt, evaluator=evaluator, \n",
    "                    estimatorParamMaps=paramGrid, numFolds=3)\n",
    "\n",
    "cv2 = CrossValidator(estimator=dt, evaluator=evaluator, \n",
    "                    estimatorParamMaps=paramGrid, numFolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step7.3 建立 CrossValidator Pipeline 物件 'cv_pipeline' 及  'cv2_pipeline'\n",
    "cv_pipeline = Pipeline(stages=[vectorAssembler,vectorIndexer ,cv])\n",
    "cv2_pipeline = Pipeline(stages=[vectorAssembler,vectorIndexer ,cv2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step7.4a 使用 cv_pipeline.fit() 以 DataFrame train_df 做交叉訓練驗證,產生模型 'cv_pipelineModel\n",
    "cv_pipelineModel = cv_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step7.4b 使用 cv2_pipeline.fit() 以 DataFrame train_df 做交叉訓練驗證,產生模型 'cv2_pipelineModel\n",
    "cv2_pipelineModel = cv2_pipeline.fit(train_df)   #需要一段長時間,請勿在課程進行中執行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "82.72608308415863"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step7.5a 評估交叉驗證 (k=3) 最佳模型 RMSE\n",
    "predictions = cv_pipelineModel.transform(test_df)\n",
    "rmse= evaluator.evaluate(predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81.98824648924268"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step7.5b 評估交叉驗證 (k=10) 最佳模型 RMSE, 進步一點點\n",
    "predictions = cv2_pipelineModel.transform(test_df)\n",
    "rmse= evaluator.evaluate(predictions)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#練習:使用隨機森林RandomForestClassifier分類器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.46412057609375"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from pyspark.ml.regression import RandomForestRegressor\n",
    "#vectorAssembler = \n",
    "#vectorIndexer = \n",
    "#rf= RandomForestRegressor(labelCol=\"cnt\",featuresCol= 'features', numTrees=20)\n",
    "#......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71.12886396206777"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "#paramGrid = \n",
    "#......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#8. 使用GBT(Gradient-Boosted Tree) 梯度提昇決策樹\n",
    "#   GTB與Random Forests 都是集合很多決策樹,不同的是訓練方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step8.1 建立GBT Regression\n",
    "#匯入 GBT Regression 模組\n",
    "from pyspark.ml.regression import GBTRegressor\n",
    "#建立 GBTRegressor 物件 'gbt' , label='cnt', feature vector='features'\n",
    "gbt = GBTRegressor(labelCol=\"cnt\",featuresCol= 'features')\n",
    "#建立 Pipeline 物件 'gbt_pipeline'\n",
    "gbt_pipeline = Pipeline(stages=[vectorAssembler,vectorIndexer,gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.12550201832394"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step8.2 訓練與評估\n",
    "#以gbt_pipeline.fit() , train_df 訓練, 結果是 Model 'gbt_pipelineModel'\n",
    "gbt_pipelineModel = gbt_pipeline.fit(train_df)\n",
    "#以gbt_pipelineModel.transform(), test_df, 預測結果為 predicted_df\n",
    "predicted_df=gbt_pipelineModel.transform(test_df)\n",
    "#評估 rmse\n",
    "rmse = evaluator.evaluate(predicted_df)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step8.3 使用交叉驗證(CrossValidation) GBT Regression 找出最佳模型 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#匯入所需模組,CrossValidator, ParamGridBuilder, RegressionEvaluator, Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "#以 ParamGridBuilder()  建立參數Grid 'paramGrid'\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "  .addGrid(gbt.maxDepth, [ 5,10])\\\n",
    "  .addGrid(gbt.maxBins, [25,40])\\\n",
    "  .addGrid(gbt.maxIter, [10, 50])\\\n",
    "  .build()\n",
    "\n",
    "#建立 CrossValidator 物件 'cv' k=3\n",
    "cv = CrossValidator(estimator=gbt, evaluator=evaluator, \n",
    "                                  estimatorParamMaps=paramGrid, numFolds=3)\n",
    "\n",
    "#建立 Pipeline 物件 cv_pipeline\n",
    "cv_pipeline = Pipeline(stages=[vectorAssembler, vectorIndexer, cv])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#step8.4 以cv_pipeline.fit() , train_df 訓練資料,執行交叉驗證,結果為Model 'cv_pipelineModel'\n",
    "cv_pipelineModel = cv_pipeline.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeRegressionModel (uid=DecisionTreeRegressor_40adba5c5a1ddeb66291) of depth 10 with 1753 nodes\n",
      "  If (feature 2 in {0.0,1.0,2.0,3.0,4.0,5.0,6.0,22.0,23.0})\n",
      "   If (feature 2 in {0.0,1.0,2.0,3.0,4.0,5.0})\n",
      "    If (feature 2 in {2.0,3.0,4.0,5.0})\n",
      "     If (feature 2 in {3.0,4.0})\n",
      "      If (feature 4 in {1.0,2.0,3.0,4.0,5.0})\n",
      "       If (feature 1 in {0.0,1.0,2.0,3.0,11.0})\n",
      "        If (feature 7 <= 0.4)\n",
      "         If (feature 0 in {0.0,1.0})\n",
      "          If (feature 7 <= 0.2)\n",
      "           If (feature\n"
     ]
    }
   ],
   "source": [
    "#step8.5 檢視最佳模型的規則\n",
    "cvm=cv_pipelineModel.stages[2] \n",
    "gbestModel=cvm.bestModel\n",
    "print bestModel.toDebugString[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "|season|mnth| hr|holiday|weekday|workingday|weathersit|temp| atemp| hum|windspeed| cnt|        prediction|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "|   1.0| 1.0|0.0|    0.0|    0.0|       0.0|       1.0|0.16|0.1818| 0.8|   0.1045|33.0| 44.50534466103074|\n",
      "|   1.0| 1.0|0.0|    0.0|    1.0|       1.0|       1.0|0.06|0.0606|0.41|    0.194| 7.0| 4.012207749638969|\n",
      "|   1.0| 1.0|0.0|    0.0|    1.0|       1.0|       1.0|0.22| 0.197|0.44|   0.3582| 5.0|18.002747653346976|\n",
      "|   1.0| 1.0|0.0|    0.0|    1.0|       1.0|       1.0|0.24|0.2273| 0.6|   0.2239|15.0| 16.06004975059944|\n",
      "|   1.0| 1.0|0.0|    0.0|    1.0|       1.0|       2.0|0.32|0.2879|0.26|   0.4179|10.0| 8.355974651245623|\n",
      "|   1.0| 1.0|0.0|    0.0|    2.0|       1.0|       2.0|0.22|0.2424|0.87|   0.1045|14.0|11.050031812678764|\n",
      "|   1.0| 1.0|0.0|    0.0|    3.0|       1.0|       1.0|0.08|0.0606|0.42|   0.3284| 9.0| 6.761295846898887|\n",
      "|   1.0| 1.0|0.0|    0.0|    3.0|       1.0|       1.0|0.26| 0.303|0.93|      0.0|31.0| 18.40862427296849|\n",
      "|   1.0| 1.0|0.0|    0.0|    3.0|       1.0|       2.0|0.22|0.2273|0.69|   0.1343|17.0| 18.90142345622113|\n",
      "|   1.0| 1.0|0.0|    0.0|    4.0|       1.0|       2.0|0.28|0.2879|0.75|   0.1045|25.0|   27.927568031299|\n",
      "+------+----+---+-------+-------+----------+----------+----+------+----+---------+----+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#step8.6 預測\n",
    "#以 cv_piplelineModel.transform() , DataFrame test_df 作預測, 預測結果為 'predicted'\n",
    "predicted_df=cv_pipelineModel.transform(test_df)\n",
    "#檢視預測結果\n",
    "predicted_df.select('season', 'mnth', 'hr', 'holiday', 'weekday', 'workingday', \\\n",
    "                     'weathersit', 'temp', 'atemp', 'hum', 'windspeed','cnt','prediction').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70.94139281419096"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#step8.7 評估, 計算 RMSE\n",
    "evaluator = RegressionEvaluator(metricName=\"rmse\", \n",
    "                                labelCol='cnt', predictionCol='prediction')\n",
    "rmse = evaluator.evaluate(predicted_df)\n",
    "rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
