{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ch19_001_20171009.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create SparkSession \"spark01\"\n",
    "spark01 = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df01 = spark01.read.json(\"people.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+------+\n",
      "| age|   name|status|\n",
      "+----+-------+------+\n",
      "|null|Michael|  null|\n",
      "|  30|   Andy|  null|\n",
      "|  19| Justin|  null|\n",
      "|  38|   Jack|     1|\n",
      "|  32|  Angel|     0|\n",
      "|  18|Sabrina|     1|\n",
      "+----+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df01.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df02=spark.read.csv(\"test99.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|_c0|_c1|_c2|\n",
      "+---+---+---+\n",
      "|  1|  1|5.0|\n",
      "|  1|  2|1.0|\n",
      "|  1|  3|5.0|\n",
      "|  1|  4|1.0|\n",
      "|  2|  1|5.0|\n",
      "|  2|  2|1.0|\n",
      "|  2|  3|5.0|\n",
      "|  2|  4|1.0|\n",
      "|  3|  1|1.0|\n",
      "|  3|  2|5.0|\n",
      "|  3|  3|1.0|\n",
      "|  3|  4|5.0|\n",
      "|  4|  1|1.0|\n",
      "|  4|  2|5.0|\n",
      "|  4|  3|1.0|\n",
      "|  4|  4|5.0|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df02.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#spark01.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- status: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the schema in a tree format\n",
    "df01.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "|   Jack|\n",
      "|  Angel|\n",
      "|Sabrina|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select only the \"name\" column\n",
    "df01.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+\n",
      "|   name|(age + 1)|\n",
      "+-------+---------+\n",
      "|Michael|     null|\n",
      "|   Andy|       31|\n",
      "| Justin|       20|\n",
      "|   Jack|       39|\n",
      "|  Angel|       33|\n",
      "|Sabrina|       19|\n",
      "+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df01.select(df01.name,df01.age+1).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+\n",
      "|   name|status|\n",
      "+-------+------+\n",
      "|Michael|  null|\n",
      "|   Andy|  null|\n",
      "| Justin|  null|\n",
      "|   Jack|     1|\n",
      "|  Angel|     0|\n",
      "|Sabrina|     1|\n",
      "+-------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df01.select(df01[\"name\"],df01[\"status\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df02.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Register the DataFrame as a SQL temporary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df01.createOrReplaceTempView(\"people\") #將DataFrame df01 登錄為 TempView \"people\", Spark SQL 可叫用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df02.createTempView(\"testData\") #將DataFrame df02 登錄為 TempView \"testData\", Spark SQL 可叫用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---+\n",
      "|_c0|_c1|_c2|\n",
      "+---+---+---+\n",
      "|  1|  1|5.0|\n",
      "|  1|  2|1.0|\n",
      "|  1|  3|5.0|\n",
      "|  1|  4|1.0|\n",
      "|  2|  1|5.0|\n",
      "|  2|  2|1.0|\n",
      "|  2|  3|5.0|\n",
      "|  2|  4|1.0|\n",
      "|  3|  1|1.0|\n",
      "|  3|  2|5.0|\n",
      "|  3|  3|1.0|\n",
      "|  3|  4|5.0|\n",
      "|  4|  1|1.0|\n",
      "|  4|  2|5.0|\n",
      "|  4|  3|1.0|\n",
      "|  4|  4|5.0|\n",
      "+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark01.sql(\"SELECT * FROM testData\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|_c0|\n",
      "+---+\n",
      "|  1|\n",
      "|  1|\n",
      "|  1|\n",
      "|  1|\n",
      "|  2|\n",
      "|  2|\n",
      "|  2|\n",
      "|  2|\n",
      "|  3|\n",
      "|  3|\n",
      "|  3|\n",
      "|  3|\n",
      "|  4|\n",
      "|  4|\n",
      "|  4|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark01.sql(\"select _c0 from testData\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "|age| name|status|\n",
      "+---+-----+------+\n",
      "| 30| Andy|  null|\n",
      "| 38| Jack|     1|\n",
      "| 32|Angel|     0|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select people older than 21, DataFrame API, DataFrame.filter() \n",
    "df01.filter(df01['age'] > 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+------+\n",
      "|age| name|status|\n",
      "+---+-----+------+\n",
      "| 30| Andy|  null|\n",
      "| 38| Jack|     1|\n",
      "| 32|Angel|     0|\n",
      "+---+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select people older than 21, Spark SQL\n",
    "spark01.sql(\"select * from people where age > 21\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|status|count|\n",
      "+------+-----+\n",
      "|     0|    1|\n",
      "|  null|    3|\n",
      "|     1|    2|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count people by status, DataFrame API, DataFrame.groupBy()\n",
    "df01.groupBy(\"status\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|status|count|\n",
      "+------+-----+\n",
      "|     0|    1|\n",
      "|  null|    3|\n",
      "|     1|    2|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Count people by status, Spark SQL\n",
    "spark01.sql(\"select status, count(*) as count from people group by status\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#SparkSession.sql(\"SQL Scipt\") 回傳的是一個 DataFrame\n",
    "df03=spark01.sql(\"select status, count(*) as count from people group by status\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|status|count|\n",
      "+------+-----+\n",
      "|     0|    1|\n",
      "|  null|    3|\n",
      "|     1|    2|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df03.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Global Temporary View, Spark 2.2 \n",
    "# Register the DataFrame as a global temporary view\n",
    "df01.createGlobalTempView(\"people01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#========================================================\n",
    "# Interoperating with RDDs\n",
    "#========================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Inferring the Schema Using Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc01=spark01.sparkContext #SparkContext Object \"sc01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.context.SparkContext"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load a text file and convert each line to a Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lines01=sc01.textFile(\"people.txt\") #RDD \"lines01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parts01=lines01.map(lambda x:x.split(\",\")) #RDD.map() --> PipelinedRDD \"parts01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(parts01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#叫用 Row function()\n",
    "people01=parts01.map(lambda x:Row(name=x[0], age=int(x[1]))) #PiplelinedRDD.map() -->RDD \"people01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(age=29, name=u'Michael'),\n",
       " Row(age=30, name=u'Andy'),\n",
       " Row(age=19, name=u'Justin'),\n",
       " Row(age=38, name=u'Jack'),\n",
       " Row(age=32, name=u'Angel'),\n",
       " Row(age=18, name=u'Sabrina')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people01.collect() #RDD \"people01\" with elements of Row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Infer the schema, and register the DataFrame as a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#使用 SparkSession.createDataFrame(RDD) 將RDD \"people01\" 轉成DataFrame \"dfPeople01\n",
    "dfPeople01=spark01.createDataFrame(people01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPeople01.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(age=29, name=u'Michael')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfPeople01.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "|Michael|\n",
      "|   Andy|\n",
      "| Justin|\n",
      "|   Jack|\n",
      "|  Angel|\n",
      "|Sabrina|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfPeople01.select(\"name\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#登錄一個 TempView \"People01\"\n",
    "dfPeople01.createTempView(\"People01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "|age|   name|\n",
      "+---+-------+\n",
      "| 29|Michael|\n",
      "| 30|   Andy|\n",
      "| 19| Justin|\n",
      "| 38|   Jack|\n",
      "| 32|  Angel|\n",
      "| 18|Sabrina|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#使用 Spark SQL\n",
    "spark01.sql(\"select * from People01\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "# The results of SQL queries are Dataframe objects.\n",
    "teenagersDF = spark01.sql(\"SELECT name FROM People01 WHERE age >= 13 AND age <= 19\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   name|\n",
      "+-------+\n",
      "| Justin|\n",
      "|Sabrina|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "teenagersDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#將DataFrame 轉為 RDD, DataFrame.rdd\n",
    "teenagersRDD=teenagersDF.rdd.map(lambda x: \"Name: \" + x.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'Name: Justin', u'Name: Sabrina']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teenagersRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
