{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ch12_ALS_51.ipynb, Collaborative Filtering- spark.ml, https://spark.apache.org/docs/2.0.0-preview/ml-collaborative-filtering.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Collaborative filtering is commonly used for recommender systems. \n",
    "#spark.ml currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors \n",
    "#that can be used to predict missing entries.\n",
    "#spark.ml uses the alternating least squares (ALS) algorithm to learn these latent factors.\n",
    "\n",
    "#https://spark.apache.org/docs/2.0.0-preview/api/python/pyspark.ml.html#pyspark.ml.recommendation.ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#The implementation in spark.ml has the following parameters:\n",
    "#numBlocks --> is the number of blocks the users and items will be partitioned into in order to parallelize computation (defaults to 10).\n",
    "#           rank --> is the number of latent factors in the model (defaults to 10).\n",
    "#     maxIter --> is the maximum number of iterations to run (defaults to 10).\n",
    "#  regParam --> specifies the regularization parameter in ALS (defaults to 1.0).\n",
    "#implicitPrefs --> specifies whether to use the explicit feedback ALS variant or one adapted for implicit feedback data\n",
    "#                          (defaults to false which means using explicit feedback).\n",
    "#        alpha  --> is a parameter applicable to the implicit feedback variant of ALS that governs the baseline confidence in preference observations \n",
    "#                         (defaults to 1.0).\n",
    "#nonnegative --> specifies whether or not to use nonnegative constraints for least squares (defaults to false)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#1. example:\n",
    "#In the following example, we load rating data from the MovieLens dataset, each row consisting of a user, a movie, a rating and a timestamp.\n",
    "#We then train an ALS model which assumes, by default, that the ratings are explicit (implicitPrefs is False). \n",
    "#We evaluate the recommendation model by measuring the root-mean-square error of rating prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#create SparkSession \"spark01\"\n",
    "from pyspark.sql import SparkSession\n",
    "spark01 = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#以 SparkSession.read.text() 讀取 text file, 轉成DataFrame \"df01\"\n",
    "df01=spark01.read.text(\"sample_movielens_ratings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df01.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "| 0::2::3::1424380312|\n",
      "| 0::3::1::1424380312|\n",
      "| 0::5::2::1424380312|\n",
      "| 0::9::4::1424380312|\n",
      "|0::11::1::1424380312|\n",
      "|0::12::2::1424380312|\n",
      "|0::15::1::1424380312|\n",
      "|0::17::1::1424380312|\n",
      "|0::19::1::1424380312|\n",
      "|0::21::1::1424380312|\n",
      "|0::23::1::1424380312|\n",
      "|0::26::3::1424380312|\n",
      "|0::27::1::1424380312|\n",
      "|0::28::1::1424380312|\n",
      "|0::29::1::1424380312|\n",
      "|0::30::1::1424380312|\n",
      "|0::31::1::1424380312|\n",
      "|0::34::1::1424380312|\n",
      "|0::37::1::1424380312|\n",
      "|0::41::2::1424380312|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df01.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#以 DataFrame.rdd 將 DataFrame \"df01\" 轉成RDD \"rawDataRDD\"\n",
    "rawDataRDD=df01.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(value=u'0::2::3::1424380312')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawDataRDD.first() #檢視 rawDataRDD 第一筆資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#以RDD.map(lambda) 將 RDD \"rawDataRDD\" 依 \"::\" 分割 map 成 RDD \"partsRDD\"\n",
    "partsRDD = rawDataRDD.map(lambda r: r.value.split(\"::\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'0', u'2', u'3', u'1424380312']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partsRDD.first() #['0','2','3','143......'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#資料型別轉換, 使用 Row type\n",
    "ratingsRDD = partsRDD.map(lambda x: Row(userId=int(x[0]), movieId=int(x[1]),\n",
    "                                     rating=float(x[2]), timestamp=long(x[3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(movieId=2, rating=3.0, timestamp=1424380312L, userId=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratingsRDD.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#最後,再以SparkSeeeion.createDataFrame(RDD), 將RDD \"ratingsRDD\" 轉回成 DataFrame \"ratingsDF\"\n",
    "ratingsDF = spark01.createDataFrame(ratingsRDD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+------+\n",
      "|movieId|rating| timestamp|userId|\n",
      "+-------+------+----------+------+\n",
      "|      2|   3.0|1424380312|     0|\n",
      "|      3|   1.0|1424380312|     0|\n",
      "|      5|   2.0|1424380312|     0|\n",
      "|      9|   4.0|1424380312|     0|\n",
      "|     11|   1.0|1424380312|     0|\n",
      "+-------+------+----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ratingsDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#於是,我們可以用 DataFrame-Based Collaborative filtering 運算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step 1: 以DataFrame.randomSplit() 將 DataFrame \"ratingsDF\" 依設定機率比例 (8:2) 分成訓練資料 \"trainingDF\" 及 測試資料 \"testDF\"\n",
    "(trainingDF, testDF) = ratingsDF.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練資料筆數: 1187 測試資料筆數: 314\n"
     ]
    }
   ],
   "source": [
    "print('訓練資料筆數: '+str(trainingDF.count())+' 測試資料筆數: '+str(testDF.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step 2:  Build the recommendation model  \" alsModel\" using ALS on the training data\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol=\"userId\", itemCol=\"movieId\", ratingCol=\"rating\") #als, Alternating Least Squares (ALS) matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#以 Estimator.fit(training Data), 訓練階段, 得到模型 alsModel\n",
    "alsModel=als.fit(trainingDF)  #alsModel, pyspark.ml.recommendation.ALSModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.recommendation.ALSModel"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(alsModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#step 3:  Evaluate the model by computing the RMSE on the test data\n",
    "#             We evaluate the recommendation model by measuring the root-mean-square error of rating prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#使用 Transformer.transform(test data), 預測test 資料 \"testDF\", 回傳的是 DataFrame \"predictionsDF\"\n",
    "predictionsDF = alsModel.transform(testDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----------+------+----------+\n",
      "|movieId|rating| timestamp|userId|prediction|\n",
      "+-------+------+----------+------+----------+\n",
      "|     31|   3.0|1424380312|     7|0.02137813|\n",
      "|     31|   1.0|1424380312|    18| 1.6260848|\n",
      "|     85|   1.0|1424380312|    12| 2.9139917|\n",
      "|     85|   3.0|1424380312|     6| 2.6702542|\n",
      "|     85|   5.0|1424380312|     8| 5.3887396|\n",
      "+-------+------+----------+------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictionsDF.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#以 RegressionEvaluator() 建立評估器 \"evaluator01\", pyspark.ml.evaluation.RegressionEvaluator\n",
    "evaluator01 = RegressionEvaluator(metricName=\"rmse\", labelCol=\"rating\",\n",
    "                                predictionCol=\"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#以 RegressionEvaluator.evaluate() 計算 rmse\n",
    "rmse = evaluator01.evaluate(predictionsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root-mean-square error = 1.89281881552\n"
     ]
    }
   ],
   "source": [
    "print(\"Root-mean-square error = \" + str(rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
